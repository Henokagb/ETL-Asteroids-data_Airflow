[[34m2024-04-29T21:58:44.237+0100[0m] {[34mtask_context_logger.py:[0m63} INFO[0m - Task context logging is enabled[0m
[[34m2024-04-29T21:58:44.238+0100[0m] {[34mexecutor_loader.py:[0m235} INFO[0m - Loaded executor: SequentialExecutor[0m
[[34m2024-04-29T21:58:44.277+0100[0m] {[34mscheduler_job_runner.py:[0m796} INFO[0m - Starting the scheduler[0m
[[34m2024-04-29T21:58:44.277+0100[0m] {[34mscheduler_job_runner.py:[0m803} INFO[0m - Processing each file at most -1 times[0m
[[34m2024-04-29T21:58:44.282+0100[0m] {[34mmanager.py:[0m170} INFO[0m - Launched DagFileProcessorManager with pid: 152945[0m
[[34m2024-04-29T21:58:44.284+0100[0m] {[34mscheduler_job_runner.py:[0m1595} INFO[0m - Adopting or resetting orphaned tasks for active dag runs[0m
[[34m2024-04-29T21:58:44.286+0100[0m] {[34msettings.py:[0m60} INFO[0m - Configured default timezone UTC[0m
[2024-04-29T21:58:44.316+0100] {manager.py:393} WARNING - Because we cannot use more than 1 thread (parsing_processes = 2) when using sqlite. So we set parallelism to 1.
[[34m2024-04-29T21:59:11.756+0100[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ETL_with_NASA_Asteroids_data.Extract_data_from_API manual__2024-04-29T20:59:10.642377+00:00 [scheduled]>[0m
[[34m2024-04-29T21:59:11.756+0100[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG ETL_with_NASA_Asteroids_data has 0/16 running and queued tasks[0m
[[34m2024-04-29T21:59:11.756+0100[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ETL_with_NASA_Asteroids_data.Extract_data_from_API manual__2024-04-29T20:59:10.642377+00:00 [scheduled]>[0m
[[34m2024-04-29T21:59:11.760+0100[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='ETL_with_NASA_Asteroids_data', task_id='Extract_data_from_API', run_id='manual__2024-04-29T20:59:10.642377+00:00', try_number=1, map_index=-1) to executor with priority 3 and queue default[0m
[[34m2024-04-29T21:59:11.760+0100[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ETL_with_NASA_Asteroids_data', 'Extract_data_from_API', 'manual__2024-04-29T20:59:10.642377+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py'][0m
[[34m2024-04-29T21:59:11.765+0100[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ETL_with_NASA_Asteroids_data', 'Extract_data_from_API', 'manual__2024-04-29T20:59:10.642377+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py'][0m
[[34m2024-04-29T21:59:13.071+0100[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/h3n0k/Documents/Repositories/Perso/ETL-Asteroids_data/airflow/dags/etl_dag.py[0m
[[34m2024-04-29T21:59:13.824+0100[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: ETL_with_NASA_Asteroids_data.Extract_data_from_API manual__2024-04-29T20:59:10.642377+00:00 [queued]> on host h3n0k-HP-ProBook-450-G7[0m
[[34m2024-04-29T21:59:17.538+0100[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ETL_with_NASA_Asteroids_data', task_id='Extract_data_from_API', run_id='manual__2024-04-29T20:59:10.642377+00:00', try_number=1, map_index=-1)[0m
[[34m2024-04-29T21:59:17.549+0100[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=ETL_with_NASA_Asteroids_data, task_id=Extract_data_from_API, run_id=manual__2024-04-29T20:59:10.642377+00:00, map_index=-1, run_start_date=2024-04-29 20:59:13.871998+00:00, run_end_date=2024-04-29 20:59:17.079896+00:00, run_duration=3.207898, state=success, executor_state=success, try_number=1, max_tries=1, job_id=10, pool=default_pool, queue=default, priority_weight=3, operator=PythonOperator, queued_dttm=2024-04-29 20:59:11.757611+00:00, queued_by_job_id=9, pid=153073[0m
[[34m2024-04-29T21:59:18.245+0100[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ETL_with_NASA_Asteroids_data.Transform_and_clean_data manual__2024-04-29T20:59:10.642377+00:00 [scheduled]>[0m
[[34m2024-04-29T21:59:18.246+0100[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG ETL_with_NASA_Asteroids_data has 0/16 running and queued tasks[0m
[[34m2024-04-29T21:59:18.246+0100[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ETL_with_NASA_Asteroids_data.Transform_and_clean_data manual__2024-04-29T20:59:10.642377+00:00 [scheduled]>[0m
[[34m2024-04-29T21:59:18.248+0100[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='ETL_with_NASA_Asteroids_data', task_id='Transform_and_clean_data', run_id='manual__2024-04-29T20:59:10.642377+00:00', try_number=1, map_index=-1) to executor with priority 2 and queue default[0m
[[34m2024-04-29T21:59:18.248+0100[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ETL_with_NASA_Asteroids_data', 'Transform_and_clean_data', 'manual__2024-04-29T20:59:10.642377+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py'][0m
[[34m2024-04-29T21:59:18.252+0100[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ETL_with_NASA_Asteroids_data', 'Transform_and_clean_data', 'manual__2024-04-29T20:59:10.642377+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py'][0m
[[34m2024-04-29T21:59:19.585+0100[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/h3n0k/Documents/Repositories/Perso/ETL-Asteroids_data/airflow/dags/etl_dag.py[0m
[[34m2024-04-29T21:59:20.159+0100[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: ETL_with_NASA_Asteroids_data.Transform_and_clean_data manual__2024-04-29T20:59:10.642377+00:00 [queued]> on host h3n0k-HP-ProBook-450-G7[0m
[[34m2024-04-29T21:59:20.999+0100[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ETL_with_NASA_Asteroids_data', task_id='Transform_and_clean_data', run_id='manual__2024-04-29T20:59:10.642377+00:00', try_number=1, map_index=-1)[0m
[[34m2024-04-29T21:59:21.007+0100[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=ETL_with_NASA_Asteroids_data, task_id=Transform_and_clean_data, run_id=manual__2024-04-29T20:59:10.642377+00:00, map_index=-1, run_start_date=2024-04-29 20:59:20.212021+00:00, run_end_date=2024-04-29 20:59:20.622140+00:00, run_duration=0.410119, state=success, executor_state=success, try_number=1, max_tries=1, job_id=11, pool=default_pool, queue=default, priority_weight=2, operator=PythonOperator, queued_dttm=2024-04-29 20:59:18.247502+00:00, queued_by_job_id=9, pid=153105[0m
[[34m2024-04-29T21:59:21.060+0100[0m] {[34mscheduler_job_runner.py:[0m417} INFO[0m - 1 tasks up for execution:
	<TaskInstance: ETL_with_NASA_Asteroids_data.Load_into_db manual__2024-04-29T20:59:10.642377+00:00 [scheduled]>[0m
[[34m2024-04-29T21:59:21.061+0100[0m] {[34mscheduler_job_runner.py:[0m480} INFO[0m - DAG ETL_with_NASA_Asteroids_data has 0/16 running and queued tasks[0m
[[34m2024-04-29T21:59:21.061+0100[0m] {[34mscheduler_job_runner.py:[0m596} INFO[0m - Setting the following tasks to queued state:
	<TaskInstance: ETL_with_NASA_Asteroids_data.Load_into_db manual__2024-04-29T20:59:10.642377+00:00 [scheduled]>[0m
[[34m2024-04-29T21:59:21.062+0100[0m] {[34mscheduler_job_runner.py:[0m639} INFO[0m - Sending TaskInstanceKey(dag_id='ETL_with_NASA_Asteroids_data', task_id='Load_into_db', run_id='manual__2024-04-29T20:59:10.642377+00:00', try_number=1, map_index=-1) to executor with priority 1 and queue default[0m
[[34m2024-04-29T21:59:21.063+0100[0m] {[34mbase_executor.py:[0m149} INFO[0m - Adding to queue: ['airflow', 'tasks', 'run', 'ETL_with_NASA_Asteroids_data', 'Load_into_db', 'manual__2024-04-29T20:59:10.642377+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py'][0m
[[34m2024-04-29T21:59:21.069+0100[0m] {[34msequential_executor.py:[0m74} INFO[0m - Executing command: ['airflow', 'tasks', 'run', 'ETL_with_NASA_Asteroids_data', 'Load_into_db', 'manual__2024-04-29T20:59:10.642377+00:00', '--local', '--subdir', 'DAGS_FOLDER/etl_dag.py'][0m
[[34m2024-04-29T21:59:22.313+0100[0m] {[34mdagbag.py:[0m540} INFO[0m - Filling up the DagBag from /home/h3n0k/Documents/Repositories/Perso/ETL-Asteroids_data/airflow/dags/etl_dag.py[0m
[[34m2024-04-29T21:59:22.848+0100[0m] {[34mtask_command.py:[0m426} INFO[0m - Running <TaskInstance: ETL_with_NASA_Asteroids_data.Load_into_db manual__2024-04-29T20:59:10.642377+00:00 [queued]> on host h3n0k-HP-ProBook-450-G7[0m
[[34m2024-04-29T21:59:23.547+0100[0m] {[34mscheduler_job_runner.py:[0m689} INFO[0m - Received executor event with state success for task instance TaskInstanceKey(dag_id='ETL_with_NASA_Asteroids_data', task_id='Load_into_db', run_id='manual__2024-04-29T20:59:10.642377+00:00', try_number=1, map_index=-1)[0m
[[34m2024-04-29T21:59:23.553+0100[0m] {[34mscheduler_job_runner.py:[0m721} INFO[0m - TaskInstance Finished: dag_id=ETL_with_NASA_Asteroids_data, task_id=Load_into_db, run_id=manual__2024-04-29T20:59:10.642377+00:00, map_index=-1, run_start_date=2024-04-29 20:59:22.892690+00:00, run_end_date=2024-04-29 20:59:23.122408+00:00, run_duration=0.229718, state=success, executor_state=success, try_number=1, max_tries=1, job_id=12, pool=default_pool, queue=default, priority_weight=1, operator=PythonOperator, queued_dttm=2024-04-29 20:59:21.061847+00:00, queued_by_job_id=9, pid=153118[0m
[[34m2024-04-29T21:59:23.598+0100[0m] {[34mdagrun.py:[0m851} INFO[0m - Marking run <DagRun ETL_with_NASA_Asteroids_data @ 2024-04-29 20:59:10.642377+00:00: manual__2024-04-29T20:59:10.642377+00:00, state:running, queued_at: 2024-04-29 20:59:10.676025+00:00. externally triggered: True> successful[0m
[[34m2024-04-29T21:59:23.599+0100[0m] {[34mdagrun.py:[0m902} INFO[0m - DagRun Finished: dag_id=ETL_with_NASA_Asteroids_data, execution_date=2024-04-29 20:59:10.642377+00:00, run_id=manual__2024-04-29T20:59:10.642377+00:00, run_start_date=2024-04-29 20:59:11.680137+00:00, run_end_date=2024-04-29 20:59:23.599177+00:00, run_duration=11.91904, state=success, external_trigger=True, run_type=manual, data_interval_start=2024-04-22 20:59:10.642377+00:00, data_interval_end=2024-04-29 20:59:10.642377+00:00, dag_hash=e8548d8910942cd168dbeeaf021bfc57[0m
[[34m2024-04-29T22:00:08.953+0100[0m] {[34mscheduler_job_runner.py:[0m256} INFO[0m - Exiting gracefully upon receiving signal 15[0m
[[34m2024-04-29T22:00:09.969+0100[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 152945. PIDs of all processes in the group: [152945][0m
[[34m2024-04-29T22:00:09.969+0100[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 152945[0m
[[34m2024-04-29T22:00:10.222+0100[0m] {[34mprocess_utils.py:[0m80} INFO[0m - Process psutil.Process(pid=152945, status='terminated', exitcode=0, started='21:58:44') (152945) terminated with exit code 0[0m
[[34m2024-04-29T22:00:10.239+0100[0m] {[34mprocess_utils.py:[0m132} INFO[0m - Sending Signals.SIGTERM to group 152945. PIDs of all processes in the group: [][0m
[[34m2024-04-29T22:00:10.239+0100[0m] {[34mprocess_utils.py:[0m87} INFO[0m - Sending the signal Signals.SIGTERM to group 152945[0m
[[34m2024-04-29T22:00:10.240+0100[0m] {[34mprocess_utils.py:[0m101} INFO[0m - Sending the signal Signals.SIGTERM to process 152945 as process group is missing.[0m
[[34m2024-04-29T22:00:10.240+0100[0m] {[34mscheduler_job_runner.py:[0m872} INFO[0m - Exited execute loop[0m
